{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# pyDataverse Data Migration Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**European Dataverse Workshop 2020 @ Tromso**\n",
    "\n",
    "This Jupyter Notebook is part of the [European Dataverse Workshop 2020 at Tromso](https://github.com/AUSSDA/pyDataverse_workshop_tromso). It shows a demo showcase on how to do data migrations into Dataverse with [pyDataverse](https://github.com/AUSSDA/pyDataverse).\n",
    "\n",
    "This Jupyter Notebook is used as an executable migration script with documentation.\n",
    "\n",
    "* Date: 24th January 2020\n",
    "* Location: [UiT - The Arctic University of Norway](https://en.uit.no/startsida), Troms√∏\n",
    "* Trainer: Stefan Kasberger from [AUSSDA - The Austrian Social Science Data Archive](https://aussda.at).\n",
    "* Materials: [GitHub Repository](https://github.com/AUSSDA/pyDataverse_workshop_tromso)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* [Dataverse Docker](https://github.com/IQSS/dataverse-docker) (commit: [19b2e86](https://github.com/IQSS/dataverse-docker/commit/19b2e86bdd32a49f4c5940e284844b6d5bf99570))\n",
    "* [Jupyter Docker](https://hub.docker.com/r/jupyter/datascience-notebook) (digest: [18ef2702c6a2](https://hub.docker.com/layers/jupyter/datascience-notebook/latest/images/sha256-18ef2702c6a25bd26b81e7b6dc831adb2bc294ae7bc9b011150b8f4573c41d4a))\n",
    "* [pyDataverse](https://github.com/AUSSDA/pyDataverse) (branch: [develop](https://github.com/AUSSDA/pyDataverse/tree/develop))\n",
    "\n",
    "**Overview**\n",
    "\n",
    "1. Short introduction into pyDataverse (DONE)\n",
    "2. Setup the user environment\n",
    "3. Introduce to pyDataverse CSV templates\n",
    "4. Import Datasets metadata into pyDataverse\n",
    "5. Upload Datasets to Dataverse\n",
    "6. Import Datafiles metadata into pyDataverse\n",
    "7. Upload Datafiles to Dataverse\n",
    "8. Delete Datasets (optional)\n",
    "9. Copy Jupyter Notebook to localhost (optional)\n",
    "\n",
    "**Software Architecture**\n",
    "\n",
    "![Software architecture](assets/architecture.png)\n",
    "\n",
    "\n",
    "**pyDataverse Workflow**\n",
    "\n",
    "![pyDataverse Workflow](assets/flow-chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction pyDataverse\n",
    "\n",
    "See [presentation.pdf](https://github.com/AUSSDA/pyDataverse_workshop_tromso/presentation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Before to start, we need to setup our working environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Dataverse (Docker)**\n",
    "\n",
    "If not already running, install and start your Dataverse Docker container. Find out more about this inside its [Dataverse Docker GitHub repository](https://github.com/IQSS/dataverse-docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Jupyter Notebook (Docker)**\n",
    "\n",
    "Download and start Docker container for Jupyter notebook ([jupyter/datascience-notebook](https://hub.docker.com/r/jupyter/datascience-notebook)).\n",
    "\n",
    "```shell\n",
    "$ docker pull jupyter/datascience-notebook@sha256:18ef2702c6a25bd26b81e7b6dc831adb2bc294ae7bc9b011150b8f4573c41d4a\n",
    "$ docker run -p 8888:8888 jupyter/scipy-notebook\n",
    "```\n",
    "\n",
    "![Start Jupyter Notebook](assets/screenshot_start-jupyter-container.png)\n",
    "\n",
    "Now you can run the Jupyter Notebook environment by clicking the link with the token outputed to your terminal. This should open a window in your Browser, where you see the container home directory with the folder `work/`.\n",
    "\n",
    "![Empty Notebook](assets/screenshot_empty-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go inside the Jupyter container**\n",
    "\n",
    "To be able to install pyDataverse and download the demo repositorium, we need to go inside the the container first. To get in, `docker ps` lists up all the running Docker container. Look for the `jupyter/scipy-notebook` container ID and copy it. Then paste it instead of `CONTAINER_ID`in `docker exec -it CONTAINER_ID bash`.\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker exec -it CONTAINER_ID bash\n",
    "```\n",
    "\n",
    "This should get you inside the shell of the Jupyter Notebook container.\n",
    "\n",
    "![Go into Jupyter Docker container](assets/screenshot_get-into-docker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Jupyter environment**\n",
    "\n",
    "Once you are inside, you can install [pydataverse](https://github.com/AUSSDA/pyDataverse). To have a working version, we install it from the commit [fbe9755](https://github.com/AUSSDA/pyDataverse/commit/fbe9755466556f5dd5044bbc732fac0d80a1dc38).\n",
    "\n",
    "```shell\n",
    "$ pip install git+https://github.com/aussda/pyDataverse.git@fbe9755466556f5dd5044bbc732fac0d80a1dc38\n",
    "```\n",
    "\n",
    "![Install pyDataverse](assets/screenshot_install-pydataverse_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clone the demo repository**\n",
    "\n",
    "Finally, download the [GitHub Repository](https://github.com/AUSSDA/pyDataverse_demo_tromso) for this demo. It contains all needed scripts, data and files for this demo.\n",
    "\n",
    "```shell\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_demo_tromso.git\n",
    "```\n",
    "\n",
    "![Clone Workshop repository](assets/screenshot_clone-repo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is installed and up and running, so we can get our hands on some real Dataverse data.\n",
    "\n",
    "Go back into your Browser. In the Jupyter Notebook Dashboard, the `pyDataverse_demo_tromso/` folder can be seen. Go into it and open the `pydataverse.ipynb` file by clicking on it.\n",
    "\n",
    "![Workshop in Jupyter](assets/screenshot_workshop-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets and Datafiles templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data import approach in this demo, is to use the [pyDataverse CSV templates](https://github.com/AUSSDA/pyDataverse_templates) and its structure. The two needed files for the test data migration are already prepared in our GitHub repository. You can find them as `datasets.csv` and `datafiles.csv` in the GitHub repository directory.\n",
    "\n",
    "* [datasets.csv](https://github.com/AUSSDA/pyDataverse_demo_tromso/blob/master/datasets.csv)\n",
    "* [datafiles.csv](https://github.com/AUSSDA/pyDataverse_demo_tromso/blob/master/datafiles.csv)\n",
    "\n",
    "The general concept of Datasets and Datafiles is, that a Dataset can contain multiple Datafiles. The relation between these two variables is established through the variable `organization.dataset_id`. It is included in both CSV files and connects every single Datafile with its parent Dataset.\n",
    "\n",
    "To create your own CSV files with your own metadata inside, the use of the [pyDataverse templates](https://github.com/AUSSDA/pyDataverse_templates) is recommended.\n",
    "\n",
    "**datasets.csv**\n",
    "\n",
    "[![datasets.csv](assets/screenshot_datasets.png)](https://github.com/AUSSDA/pyDataverse_demo_tromso/blob/master/datasets.csv)\n",
    "\n",
    "**datafiles.csv**\n",
    "\n",
    "[![datafiles.csv](assets/screenshot_datafiles.png)](https://github.com/AUSSDA/pyDataverse_demo_tromso/blob/master/datafiles.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Datasets metadata from template into pyDataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the needed Python modules.\n",
    "2. Import CSV to a Python dictionary\n",
    "3. Create the pyDataverse `Dataset()` object\n",
    "4. Import Dataset dictionary into `Dataset()`\n",
    "5. Print out `Dataset()` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import json\n",
    "from pyDataverse.api import Api\n",
    "from pyDataverse.models import Datafile\n",
    "from pyDataverse.models import Dataset\n",
    "from pyDataverse.utils import read_csv_to_dict\n",
    "from pyDataverse.utils import read_file\n",
    "from demo import create_dataset\n",
    "from demo import delete_dataset\n",
    "from demo import import_datafile\n",
    "from demo import parse_dataset_keys\n",
    "from demo import publish_dataset\n",
    "from demo import upload_datafile\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filename = 'datasets.csv'\n",
    "license_filename = 'license.html'\n",
    "terms_filename = 'terms-of-access.html'\n",
    "\n",
    "data = {}\n",
    "license_default = read_file(license_filename)\n",
    "datasets_csv = read_csv_to_dict(ds_filename, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets metadata from CSV file and save it in a dictionary\n",
    "\n",
    "for dataset in datasets_csv:\n",
    "    data = parse_dataset_keys(dataset, data, terms_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyDataverse Dataset object and import data from dictionary\n",
    "\n",
    "ds_1 = Dataset()\n",
    "ds_1.set(data['test_1']['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet usage 2019\n",
      "Life Style 2019. Internet usage / media.\n"
     ]
    }
   ],
   "source": [
    "# Print out some basic metadata\n",
    "\n",
    "print(ds_1.title)\n",
    "print(ds_1.dsDescription[0]['dsDescriptionValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Dataset Metadata via API\n",
    "\n",
    "1. Get API token\n",
    "2. Connect to Dataverse API with pyDataverse\n",
    "3. Upload all `Dataset()` to Dataverse via API\n",
    "4. View PID's of the uploaded Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get an API token for the Dataverse API. \n",
    "\n",
    "* Go to [Dataverse API token page](http://localhost:8085/dataverseuser.xhtml?selectTab=apiTokenTab).\n",
    "* Create your own API token.\n",
    "* Assign the API token to the `API_TOKEN` variable below, instead of the value `SECRET`, and uncomment the line (remove the \"#\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_alias = 'root'\n",
    "BASE_URL = 'http://localhost:8085'\n",
    "API_TOKEN = '7ab830e0-a493-4e76-96c4-901d4b7ef2bf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect with API\n",
    "\n",
    "api = Api(BASE_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with pid 'doi:10.5072/FK2/YQGUXH' created.\n",
      "http://localhost:8085/dataset.xhtml?persistentId=doi:10.5072/FK2/YQGUXH&version=DRAFT\n",
      "Dataset with pid 'doi:10.5072/FK2/Y1Y6L3' created.\n",
      "http://localhost:8085/dataset.xhtml?persistentId=doi:10.5072/FK2/Y1Y6L3&version=DRAFT\n"
     ]
    }
   ],
   "source": [
    "# upload Dataset metadata via API\n",
    "\n",
    "mapping_dsid2pid = {}\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    ds = Dataset()\n",
    "    ds.set(dataset['metadata'])\n",
    "    resp, mapping_dsid2pid = create_dataset(api, ds, dv_alias, mapping_dsid2pid, ds_id, BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_1': 'doi:10.5072/FK2/YQGUXH', 'test_2': 'doi:10.5072/FK2/Y1Y6L3'}\n"
     ]
    }
   ],
   "source": [
    "# Print out mapping from Dataset ID to DOI\n",
    "\n",
    "print(mapping_dsid2pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Datafiles metadata from template into pyDataverse\n",
    "\n",
    "1. Import CSV to a Python dictionary\n",
    "2. Create the pyDataverse `Datafile()` object\n",
    "3. Import Datafile dictionary into `Datafile()`\n",
    "4. Print out attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datafile metadata from CSV and save it in the dictionary.\n",
    "\n",
    "df_filename = 'datafiles.csv'\n",
    "datafiles_csv = read_csv_to_dict(df_filename, delimiter=',')\n",
    "\n",
    "for datafile in datafiles_csv:\n",
    "    data = import_datafile(datafile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyDataverse Datafile object and import data from dictionary\n",
    "\n",
    "df_1 = Datafile()\n",
    "df_1.set(data['test_1']['datafiles']['1']['metadata'])\n",
    "df_1.set({'pid': mapping_dsid2pid['test_1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doi:10.5072/FK2/YQGUXH\n",
      "20001_ta_de_v1_0.tsv\n"
     ]
    }
   ],
   "source": [
    "# Print out some basic metadata\n",
    "\n",
    "print(df_1.pid)\n",
    "print(df_1.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Datafiles metadata and data to Dataverse via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload Datafile metadata and data via API\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    for df_id, datafile in dataset['datafiles'].items():\n",
    "        data_tmp = datafile['metadata']\n",
    "        data_tmp['pid'] = pid\n",
    "        df = Datafile()\n",
    "        df.set(data_tmp)\n",
    "        filename = os.path.abspath(os.path.join('data', datafile['metadata']['filename']))\n",
    "        resp = upload_datafile(api, pid, filename, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delete Datasets via API (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Datasets at the End (OPTIONAL)\n",
    "DELETE_DATASETS = False\n",
    "\n",
    "if DELETE_DATASETS:\n",
    "    for ds_id, dataset in data.items():\n",
    "        resp = delete_dataset(mapping_dsid2pid[ds_id], api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Copy Jupyter Notebook to localhost (optional)\n",
    "\n",
    "Files from within the Docker container can get out via the docker `cp` command. For this you must have the Container ID of the Jupyter Notebook running and pass the local folder in which you want the file to be copied.\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker cp CONTAINER_ID:/home/jovyan/work/pyDataverse_demo_tromso/pydataverse.ipynb YOUR_LOCAL_DIRECTORY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Dataverse API Docs](http://guides.dataverse.org/en/latest/api/index.html)\n",
    "* [pyDataverse templates](https://github.com/AUSSDA/pyDataverse_templates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
