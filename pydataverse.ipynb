{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demo: Use pyDataverse for data migrations into Dataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**European Dataverse Workshop @ Tromso**\n",
    "\n",
    "This Jupyter Notebook is part of the European Dataverse Workhshop at Tromso. It offers a little demo, how to use [pyDataverse](https://github.com/AUSSDA/pyDataverse) for a data migration.\n",
    "\n",
    "* Date: 24th January 2020\n",
    "* Location: [UiT - The Arctic University of Norway](https://en.uit.no/startsida), Tromsø\n",
    "* Trainer: Stefan Kasberger from [AUSSDA - The Austrian Social Science Data Archive](https://aussda.at).\n",
    "* Workshop Materials: [GitHub Repository](https://github.com/AUSSDA/european-dataverse-workshop-tromso)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* [Dataverse Docker](https://github.com/IQSS/dataverse-docker)\n",
    "* [Jupyter Docker](https://hub.docker.com/r/jupyter/datascience-notebook)\n",
    "* [pyDataverse](https://github.com/AUSSDA/pyDataverse) ([develop](https://github.com/AUSSDA/pyDataverse/tree/develop))\n",
    "\n",
    "**Overview**\n",
    "\n",
    "What we will do:\n",
    "\n",
    "* Get a short introduction into the idea of pyDataverse (DONE)\n",
    "* Prepare the environment for the data migration:\n",
    "* Explain the pyDataverse templates and its usage\n",
    "* Import the data from the pyDataverse templates into pyDataverse\n",
    "* Upload the data via the API to our Dataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction pyDataverse\n",
    "\n",
    "See the [slides](https://github.com/AUSSDA/european-dataverse-workshop-tromso/presentation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your local terminal.\n",
    "\n",
    "TODO\n",
    "\n",
    "* SCREENSHOT Empty Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and start Docker container for Jupyter notebook ([jupyter/datascience-notebook](https://hub.docker.com/r/jupyter/datascience-notebook)).\n",
    "\n",
    "```shell\n",
    "$ docker run -p 8888:8888 jupyter/scipy-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go into the bash of the Docker container:\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker exec -it CONTAINER_ID bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are inside, you can install [pydataverse](https://github.com/AUSSDA/pyDataverse). To have the latest features, we install from the develop branch.\n",
    "\n",
    "```shell\n",
    "$ pip install git+https://github.com/aussda/pyDataverse.git@develop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can download the [pyDataverse templates from GitHub](https://github.com/AUSSDA/pyDataverse_templates), which are needed for the import:\n",
    "\n",
    "```shell\n",
    "$ cd work/\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_templates.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we download the [GitHub Repository for this workshop](https://github.com/AUSSDA/pyDataverse_workshop_tromso), with the Jupyter Notebook inside, prepared for this workshop.\n",
    "\n",
    "```shell\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_workshop_tromso.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open Jupyter Notebook `localhost:8888`.\n",
    "* Open `work/pyDataverse_workshop_tromso/pydataverse.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is installed and up and running, so we can move on to get our hands on some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. pyDataverse templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "* Show templates empty\n",
    "* show prepared dataset.csv and datafile.csv\n",
    "* Beziehung Datasets und Datafiles erklären\n",
    "* Text abändern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we added some data to the pyDataverse template files (datasets.csv, datafiles.csv), we can import the containing data into pyDataverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Dataset Metadata from templates to pyDataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the needed Python modules.\n",
    "2. Convert the CSV data to a Python dictionary\n",
    "3. Create the pyDataverse Dataset object\n",
    "4. Print out metadata as json string\n",
    "5. Print out specific metadata variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import json\n",
    "from pyDataverse.models import Datafile\n",
    "from pyDataverse.models import Dataset\n",
    "from pyDataverse.utils import read_csv_to_dict\n",
    "from pyDataverse.utils import read_file\n",
    "from pyDataverse.utils import read_json\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filename = 'datasets.csv'\n",
    "license_filename = 'license.html'\n",
    "terms_filename = 'terms-of-access.html'\n",
    "\n",
    "data = {}\n",
    "license_default = read_file(license_filename)\n",
    "datasets_csv = read_csv_to_dict(ds_filename)\n",
    "\n",
    "for dataset in datasets_csv:\n",
    "    ds_tmp = {}\n",
    "    \n",
    "    ds_tmp['termsOfAccess'] = read_file(terms_filename)\n",
    "    for key, val in dataset.items():\n",
    "        if not val == '':\n",
    "            if key == 'aussda.dataset_id':\n",
    "                ds_id = val\n",
    "            elif key == 'dataverse.title':\n",
    "                ds_tmp['title'] = val\n",
    "            elif key == 'dataverse.subtitle':\n",
    "                ds_tmp['subtitle'] = val\n",
    "            elif key == 'dataverse.author':\n",
    "                ds_tmp['author'] = json.loads(val)\n",
    "            elif key == 'dataverse.dsDescription':\n",
    "                ds_tmp['dsDescription'] = []\n",
    "                ds_tmp['dsDescription'].append({\n",
    "                    'dsDescriptionValue': val})\n",
    "            elif key == 'dataverse.keywordValue':\n",
    "                ds_tmp['keyword'] = json.loads(val)\n",
    "            elif key == 'dataverse.topicClassification':\n",
    "                ds_tmp['topicClassification'] = json.loads(val)\n",
    "            elif key == 'dataverse.language':\n",
    "                ds_tmp['language'] = json.loads(val)\n",
    "            elif key == 'dataverse.subject':\n",
    "                ds_tmp['subject'] = []\n",
    "                ds_tmp['subject'].append(val)\n",
    "            elif key == 'dataverse.kindOfData':\n",
    "                ds_tmp['kindOfData'] = json.loads(val)\n",
    "            elif key == 'dataverse.datasetContact':\n",
    "                ds_tmp['datasetContact'] = json.loads(val)\n",
    "    data[ds_id] = {'metadata': ds_tmp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1 = Dataset()\n",
    "ds_1.set(data['test_1']['metadata'])\n",
    "ds_2 = Dataset()\n",
    "ds_2.set(data['test_2']['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== DS 1 ==\n",
      "Title: Internet usage 2019\n",
      "Description: Life Style 2019. Internet usage / media.\n",
      "== DS 2 ==\n",
      "Title: Attitute of youth towards school and profession 2018\n",
      "Description: Attitudes of teenagers about school and job.\n"
     ]
    }
   ],
   "source": [
    "print('== DS 1 ==')\n",
    "print('Title:', ds_1.title)\n",
    "print('Description:', ds_1.dsDescription[0]['dsDescriptionValue'])\n",
    "print('== DS 2 ==')\n",
    "print('Title:', ds_2.title)\n",
    "print('Description:', ds_2.dsDescription[0]['dsDescriptionValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Dataset Metadata via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get an API token for the Dataverse API. Go to [localhost:8085]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'OK', 'data': {'id': 1, 'alias': 'root', 'name': 'Root', 'dataverseContacts': [], 'permissionRoot': True, 'description': 'The root dataverse.', 'dataverseType': 'UNCATEGORIZED', 'creationDate': '2019-03-19T08:44:01Z', 'creator': {'id': 1, 'identifier': '@dataverseAdmin', 'displayName': 'Dataverse Admin', 'firstName': 'Dataverse', 'lastName': 'Admin', 'email': 'stefan.kasberger@univie.ac.at', 'superuser': False, 'affiliation': 'AUSSDA', 'position': 'Admin', 'persistentUserId': 'dataverseAdmin', 'createdTime': '2019-03-19T08:44:01Z', 'lastLoginTime': '2020-01-20T08:56:22Z', 'lastApiUseTime': '2020-01-20T17:27:45Z', 'authenticationProviderId': 'builtin'}}}\n"
     ]
    }
   ],
   "source": [
    "# load pyDataverse functionality\n",
    "from pyDataverse.api import Api\n",
    "\n",
    "dv_alias = 'root'\n",
    "BASE_URL = 'http://localhost:8085'\n",
    "API_TOKEN = '714d16d5-c375-4051-a173-bc7d29fc0799'\n",
    "\n",
    "api = Api(BASE_URL, API_TOKEN)\n",
    "resp = api.get_dataverse(dv_alias)\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with pid 'doi:10.5072/FK2/L7I0TL' created.\n",
      "Dataset with pid 'doi:10.5072/FK2/VZI3F7' created.\n"
     ]
    }
   ],
   "source": [
    "mapping_dsid2pid = {}\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    ds = Dataset()\n",
    "    ds.set(dataset['metadata'])\n",
    "    resp = api.create_dataset(dv_alias, ds.json())\n",
    "    pid = resp.json()['data']['persistentId']\n",
    "    mapping_dsid2pid[ds_id] = pid\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* View created Dataset at localhost. BROWSER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Datafile metadata from templates to pyDataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filename = 'datafiles.csv'\n",
    "datafiles_csv = read_csv_to_dict(df_filename)\n",
    "\n",
    "for datafile in datafiles_csv:\n",
    "    df_tmp = {}\n",
    "    df_id = None\n",
    "    ds_id = None\n",
    "    for key, val in datafile.items():\n",
    "        if not val == '':\n",
    "            if key == 'dataverse.description':\n",
    "                df_tmp['description'] = val\n",
    "            elif key == 'aussda.filename':\n",
    "                df_tmp['filename'] = val\n",
    "            elif key == 'aussda.datafile_id':\n",
    "                df_tmp['datafile_id'] = val\n",
    "                df_id = val\n",
    "            elif key == 'aussda.dataset_id':\n",
    "                ds_id = val\n",
    "                df_tmp['dataset_id'] = ds_id\n",
    "            elif key == 'dataverse.categories':\n",
    "                df_tmp['categories'] = json.loads(val)\n",
    "    if 'datafiles' not in data[ds_id]:\n",
    "        data[ds_id]['datafiles'] = {}\n",
    "    if df_id not in data[ds_id]['datafiles']:\n",
    "        data[ds_id]['datafiles'][df_id] = {}\n",
    "    if 'metadata' not in data[ds_id]['datafiles'][df_id]:\n",
    "        data[ds_id]['datafiles'][df_id]['metadata'] = {}\n",
    "    data[ds_id]['datafiles'][df_id]['metadata'] = df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICT: {'description': 'Documentation: Method report and Codebook ', 'categories': ['Documentation'], 'pid': 'doi:10.5072/FK2/YCLFRP', 'filename': '20002_do_de_v1_0.pdf'}\n",
      "PID: doi:10.5072/FK2/YCLFRP\n",
      "FILENAME: 20002_do_de_v1_0.pdf\n"
     ]
    }
   ],
   "source": [
    "df_1 = Datafile()\n",
    "df_1.set(data['test_1']['datafiles']['1']['metadata'])\n",
    "df_1.set({'pid': mapping_dsid2pid['test_1']})\n",
    "\n",
    "print('DICT:', df.dict())\n",
    "print('PID:', df.pid)\n",
    "print('FILENAME:', df.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Datafile metadata via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    for df_id, datafile in dataset['datafiles'].items():\n",
    "        data_tmp = datafile['metadata']\n",
    "        data_tmp['pid'] = pid\n",
    "        df = Datafile()\n",
    "        df.set(data_tmp)\n",
    "        filename = os.path.abspath(os.path.join('data', datafile['metadata']['filename']))\n",
    "        path = api.native_api_base_url\n",
    "        path += '/datasets/:persistentId/add?persistentId={0}'.format(pid)\n",
    "        shell_command = 'curl -H \"X-Dataverse-key: {0}\"'.format(API_TOKEN)\n",
    "        shell_command += ' -X POST {0} -F file=@{1}'.format(path, filename)\n",
    "        shell_command += \" -F 'jsonData={0}'\".format(df.json())\n",
    "        result = sp.run(shell_command, shell=True, stdout=sp.PIPE)\n",
    "        if filename[-4:] == '.sav' or filename[-4:] == '.dta':\n",
    "            time.sleep(20)\n",
    "        else:\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publish Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_1': 'doi:10.5072/FK2/L7I0TL', 'test_2': 'doi:10.5072/FK2/VZI3F7'}\n"
     ]
    }
   ],
   "source": [
    "print(mapping_dsid2pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ERROR', 'message': 'This dataset may not be published due to an error when contacting the <a href=http://status.datacite.org target=\"_blank\"/> DataCite </a> Service. Please try again.'}\n",
      "{'status': 'ERROR', 'message': 'This dataset may not be published due to an error when contacting the <a href=http://status.datacite.org target=\"_blank\"/> DataCite </a> Service. Please try again.'}\n"
     ]
    }
   ],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    resp = api.publish_dataset(pid, 'major')\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delete Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "ERROR: HTTP 404 - Dataset 'doi:10.5072/FK2/L7I0TL' was not found. MSG: Dataset with Persistent ID doi:10.5072/FK2/L7I0TL not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-82a9b487a253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping_dsid2pid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyDataverse/api.py\u001b[0m in \u001b[0;36mdelete_dataset\u001b[0;34m(self, identifier, is_pid, auth)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             raise DatasetNotFoundError(\n\u001b[1;32m   1057\u001b[0m                 \u001b[0;34m'ERROR: HTTP 404 - Dataset \\'{0}\\' was not found. MSG: {1}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m                 ''.format(identifier, error_msg))\n\u001b[0m\u001b[1;32m   1059\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m405\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: ERROR: HTTP 404 - Dataset 'doi:10.5072/FK2/L7I0TL' was not found. MSG: Dataset with Persistent ID doi:10.5072/FK2/L7I0TL not found."
     ]
    }
   ],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    resp = api.delete_dataset(pid)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* View uploaded Datafiles at localhost. LINK generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Dataverse API Docs](http://guides.dataverse.org/en/latest/api/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
