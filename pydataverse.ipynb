{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demo: Use pyDataverse for data migrations into Dataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**European Dataverse Workshop @ Tromso**\n",
    "\n",
    "This Jupyter Notebook is part of the [European Dataverse Workshop at Tromso](https://github.com/AUSSDA/pyDataverse_workshop_tromso). It offers a little demo, how to use [pyDataverse](https://github.com/AUSSDA/pyDataverse) for a data migration into Dataverse. \n",
    "\n",
    "This Jupyter notebook guides the demo part as an executable script, but can also be used for other purposes after it.\n",
    "\n",
    "* Date: 24th January 2020\n",
    "* Location: [UiT - The Arctic University of Norway](https://en.uit.no/startsida), Troms√∏\n",
    "* Trainer: Stefan Kasberger from [AUSSDA - The Austrian Social Science Data Archive](https://aussda.at).\n",
    "* Workshop Materials: [GitHub Repository](https://github.com/AUSSDA/pyDataverse_workshop_tromso)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* [Dataverse Docker](https://github.com/IQSS/dataverse-docker)\n",
    "* [Jupyter Docker](https://hub.docker.com/r/jupyter/datascience-notebook)\n",
    "* [pyDataverse](https://github.com/AUSSDA/pyDataverse) (branch: [develop](https://github.com/AUSSDA/pyDataverse/tree/develop))\n",
    "\n",
    "**Overview**\n",
    "\n",
    "What we will do:\n",
    "\n",
    "1. Get a short introduction into pyDataverse (DONE)\n",
    "2. Prepare the user environment\n",
    "3. Introduce the pyDataverse CSV templates\n",
    "4. Import Datasets metadata\n",
    "5. Upload Datasets to Dataverse\n",
    "6. Import Datafiles metadata\n",
    "7. Upload Datafiles to Dataverse\n",
    "8. Publish Datasets\n",
    "9. Delete Datasets (optional)\n",
    "10. Copy Jupyter Notebook to localhost\n",
    "\n",
    "How we do it:\n",
    "\n",
    "* install a fresh Jupyter Docker container on your localhost to run the Jupyter notebook with it.\n",
    "* run the Dataverse Docker container on your local host to test the data migration via API with it.\n",
    "\n",
    "**Software architecture**\n",
    "\n",
    "![Software architecture](assets/architecture.png)\n",
    "\n",
    "**How to use this Jupyter notebook**\n",
    "\n",
    "* Execute/run a notebook cell: CTRL + ENTER\n",
    "\n",
    "**pyDataverse Workflow**\n",
    "\n",
    "![pyDataverse Workflow](assets/flow-chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction pyDataverse\n",
    "\n",
    "See [presentation.pdf](https://github.com/AUSSDA/pyDataverse_workshop_tromso/presentation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparations\n",
    "\n",
    "Before we can start, we need to install some tools and prepare our working environment on our local machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Dataverse (Docker)**\n",
    "\n",
    "If not already running, install and start your Dataverse Docker container first. Find out more about this in the [Dataverse Docker GitHub repository](https://github.com/IQSS/dataverse-docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Jupyter Notebook (Docker)**\n",
    "\n",
    "Download and start Docker container for Jupyter notebook ([jupyter/datascience-notebook](https://hub.docker.com/r/jupyter/datascience-notebook)) on your laptop.\n",
    "\n",
    "```shell\n",
    "$ docker run -p 8888:8888 jupyter/scipy-notebook\n",
    "```\n",
    "\n",
    "![Start Jupyter Notebook](assets/screenshot_start-jupyter-container.png)\n",
    "\n",
    "Now you can run your Jupyter Notebook environment by clicking the link with the token mentioned inside your shell. This should open a window in your Browser, where you see the container home directory with the folder `work/`.\n",
    "\n",
    "![Empty Notebook](assets/screenshot_empty-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go inside the Jupyter container**\n",
    "\n",
    "To be able to install pyDataverse and download the workshop repositorium, we need to go inside the containers via bash. To get in, `docker ps` lists up all the running Docker container. Look for the `jupyter/scipy-notebook` container ID to copy it. Then paste it instead of `CONTAINER_ID`in `docker exec -it CONTAINER_ID bash`.\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker exec -it CONTAINER_ID bash\n",
    "```\n",
    "\n",
    "This should get you inside the shell of the Jupyter Docker container.\n",
    "\n",
    "![Go into Jupyter Docker container](assets/screenshot_get-into-docker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup environment**\n",
    "\n",
    "Once you are inside, you can install [pydataverse](https://github.com/AUSSDA/pyDataverse). To have the latest features, we install it from the develop branch.\n",
    "\n",
    "```shell\n",
    "$ pip install git+https://github.com/aussda/pyDataverse.git@develop\n",
    "```\n",
    "\n",
    "![](assets/screenshot_install-pydataverse_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clone the Workshop repositorium**\n",
    "\n",
    "And finally, download the [pyDataverse workshop Tromso GitHub Repository](https://github.com/AUSSDA/pyDataverse_workshop_tromso). It contains all needed scripts, data and files for this workshop.\n",
    "\n",
    "```shell\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_workshop_tromso.git\n",
    "```\n",
    "\n",
    "![Clone Workshop repository](assets/screenshot_clone-repo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is installed and up and running, so we can move on to get our hands on some data.\n",
    "\n",
    "Go back into your Browser. You should now see the `pyDataverse_workshop_tromso/` folder. Get into it and open the `pydataverse.ipynb` file.\n",
    "\n",
    "![Workshop in Jupyter](assets/screenshot_workshop-notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use Jupyter Notebook**\n",
    "\n",
    "* Run/execute cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets and Datafiles templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two needed files for the test data migration are already prepared in our GitHub repository. You can find them as `datasets.csv` and `datafiles.csv` in the GitHub repository directory.\n",
    "\n",
    "* [datasets.csv](https://github.com/AUSSDA/pyDataverse_workshop_tromso/blob/master/datasets.csv)\n",
    "* [datafiles.csv](https://github.com/AUSSDA/pyDataverse_workshop_tromso/blob/master/datafiles.csv)\n",
    "\n",
    "The general concept of Datasets and Datafiles is, that a Dataset can contain multiple Datafiles. The relation between the Dataset and Datafile is established via the variable `aussda.dataset_id`. It is included in both CSV files and connect every Datafile with its related Dataset.\n",
    "\n",
    "To create your own CSV files with your own metadata inside, we recommend the use of the [pyDataverse templates](https://github.com/AUSSDA/pyDataverse_templates).\n",
    "\n",
    "**datasets.csv**\n",
    "\n",
    "[![datasets.csv](assets/screenshot_datasets.png)](https://github.com/AUSSDA/pyDataverse_workshop_tromso/blob/master/datasets.csv)\n",
    "\n",
    "**datafiles.csv**\n",
    "\n",
    "[![datafiles.csv](assets/screenshot_datafiles.png)](https://github.com/AUSSDA/pyDataverse_workshop_tromso/blob/master/datafiles.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Datasets metadata from template into pyDataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the needed Python modules.\n",
    "2. Convert the CSV data to a Python dictionary\n",
    "3. Create the pyDataverse Dataset object\n",
    "4. Print out metadata as json string\n",
    "5. Print out specific metadata variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import json\n",
    "from pyDataverse.api import Api\n",
    "from pyDataverse.models import Datafile\n",
    "from pyDataverse.models import Dataset\n",
    "from pyDataverse.utils import read_csv_to_dict\n",
    "from pyDataverse.utils import read_file\n",
    "from pyDataverse.utils import read_json\n",
    "from workshop import create_dataset\n",
    "from workshop import delete_dataset\n",
    "from workshop import import_datafile\n",
    "from workshop import parse_dataset_keys\n",
    "from workshop import publish_dataset\n",
    "from workshop import upload_datafile\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filename = 'datasets.csv'\n",
    "license_filename = 'license.html'\n",
    "terms_filename = 'terms-of-access.html'\n",
    "\n",
    "data = {}\n",
    "license_default = read_file(license_filename)\n",
    "datasets_csv = read_csv_to_dict(ds_filename, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets metadata from CSV file and save it in a dictionary\n",
    "\n",
    "for dataset in datasets_csv:\n",
    "    data = parse_dataset_keys(dataset, data, terms_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyDataverse Dataset object and import data from dictionary\n",
    "\n",
    "ds_1 = Dataset()\n",
    "ds_1.set(data['test_1']['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet usage 2019\n",
      "Life Style 2019. Internet usage / media.\n"
     ]
    }
   ],
   "source": [
    "# Print out some basic metadata\n",
    "\n",
    "print(ds_1.title)\n",
    "print(ds_1.dsDescription[0]['dsDescriptionValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Dataset Metadata via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get an API token for the Dataverse API. \n",
    "\n",
    "* Go to [Dataverse API token page](http://localhost:8085/dataverseuser.xhtml?selectTab=apiTokenTab).\n",
    "* Create your own API token.\n",
    "* Assign the API token to the `API_TOKEN` variable below, instead of the value `SECRET`, and uncomment the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_alias = 'root'\n",
    "BASE_URL = 'http://localhost:8085'\n",
    "# API_TOKEN = 'SECRET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect with API\n",
    "\n",
    "api = Api(BASE_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with pid 'doi:10.5072/FK2/1KCJLD' created.\n",
      "http://localhost:8085/dataset.xhtml?persistentId=doi:10.5072/FK2/1KCJLD&version=DRAFT\n",
      "Dataset with pid 'doi:10.5072/FK2/J5NHLQ' created.\n",
      "http://localhost:8085/dataset.xhtml?persistentId=doi:10.5072/FK2/J5NHLQ&version=DRAFT\n"
     ]
    }
   ],
   "source": [
    "# upload Dataset metadata via API\n",
    "\n",
    "mapping_dsid2pid = {}\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    ds = Dataset()\n",
    "    ds.set(dataset['metadata'])\n",
    "    resp, mapping_dsid2pid = create_dataset(api, ds, dv_alias, mapping_dsid2pid, ds_id, BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Datafiles metadata from template into pyDataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datafile metadata from CSV and save it in the dictionary.\n",
    "\n",
    "df_filename = 'datafiles.csv'\n",
    "datafiles_csv = read_csv_to_dict(df_filename, delimiter=',')\n",
    "\n",
    "for datafile in datafiles_csv:\n",
    "    data = import_datafile(datafile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyDataverse Datafile object and import data from dictionary\n",
    "\n",
    "df_1 = Datafile()\n",
    "df_1.set(data['test_1']['datafiles']['1']['metadata'])\n",
    "df_1.set({'pid': mapping_dsid2pid['test_1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doi:10.5072/FK2/1KCJLD\n",
      "20001_ta_de_v1_0.tsv\n"
     ]
    }
   ],
   "source": [
    "# Print out some basic metadata\n",
    "\n",
    "print(df_1.pid)\n",
    "print(df_1.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_1': 'doi:10.5072/FK2/1KCJLD', 'test_2': 'doi:10.5072/FK2/J5NHLQ'}\n"
     ]
    }
   ],
   "source": [
    "# Print out mapping from Dataset ID to DOI\n",
    "\n",
    "print(mapping_dsid2pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Datafiles metadata and data via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload Datafile metadata and data via API\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    for df_id, datafile in dataset['datafiles'].items():\n",
    "        data_tmp = datafile['metadata']\n",
    "        data_tmp['pid'] = pid\n",
    "        df = Datafile()\n",
    "        df.set(data_tmp)\n",
    "        filename = os.path.abspath(os.path.join('data', datafile['metadata']['filename']))\n",
    "        resp = upload_datafile(api, pid, filename, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publish Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ERROR', 'message': 'This dataset may not be published due to an error when contacting the <a href=http://status.datacite.org target=\"_blank\"/> DataCite </a> Service. Please try again.'}\n",
      "{'status': 'ERROR', 'message': 'This dataset may not be published due to an error when contacting the <a href=http://status.datacite.org target=\"_blank\"/> DataCite </a> Service. Please try again.'}\n"
     ]
    }
   ],
   "source": [
    "# Publish the Datasets\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    resp = publish_dataset(mapping_dsid2pid[ds_id], api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Delete Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Datasets at the End (OPTIONAL)\n",
    "DELETE_DATASETS = False\n",
    "\n",
    "if DELETE_DATASETS:\n",
    "    for ds_id, dataset in data.items():\n",
    "        resp = delete_dataset(mapping_dsid2pid[ds_id], api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Copy Jupyter Notebook to localhost\n",
    "\n",
    "Files from within the Docker container can get out via the docker `cp` command. For this you must have the Container ID of the Jupyter Notebook running and pass the local folder in which you want the file to be copied.\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker cp CONTAINER_ID:/home/jovyan/work/pyDataverse_workshop_tromso/pydataverse.ipynb YOUR_LOCAL_DIRECTORY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Dataverse API Docs](http://guides.dataverse.org/en/latest/api/index.html)\n",
    "* [pyDataverse templates](https://github.com/AUSSDA/pyDataverse_templates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
