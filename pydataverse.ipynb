{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demo: Use pyDataverse for data migrations into Dataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**European Dataverse Workshop @ Tromso**\n",
    "\n",
    "This Jupyter Notebook is part of the European Dataverse Workhshop at Tromso. It offers a little demo, how to use [pyDataverse](https://github.com/AUSSDA/pyDataverse) for a data migration.\n",
    "\n",
    "* Date: 24th January 2020\n",
    "* Location: [UiT - The Arctic University of Norway](https://en.uit.no/startsida), Tromsø\n",
    "* Trainer: Stefan Kasberger from [AUSSDA - The Austrian Social Science Data Archive](https://aussda.at).\n",
    "* Workshop Materials: [GitHub Repository](https://github.com/AUSSDA/european-dataverse-workshop-tromso)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* [Dataverse Docker](https://github.com/IQSS/dataverse-docker)\n",
    "* [Jupyter Docker](https://hub.docker.com/r/jupyter/datascience-notebook)\n",
    "* [pyDataverse](https://github.com/AUSSDA/pyDataverse) ([develop](https://github.com/AUSSDA/pyDataverse/tree/develop))\n",
    "\n",
    "**Overview**\n",
    "\n",
    "What we will do:\n",
    "\n",
    "* Get a short introduction into the idea of pyDataverse (DONE)\n",
    "* Prepare the environment for the data migration:\n",
    "* Explain the pyDataverse templates and its usage\n",
    "* Import the data from the pyDataverse templates into pyDataverse\n",
    "* Upload the data via the API to our Dataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction pyDataverse\n",
    "\n",
    "See the [slides](https://github.com/AUSSDA/european-dataverse-workshop-tromso/presentation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your local terminal.\n",
    "\n",
    "TODO\n",
    "\n",
    "* SCREENSHOT Empty Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and start Docker container for Jupyter notebook ([jupyter/datascience-notebook](https://hub.docker.com/r/jupyter/datascience-notebook)).\n",
    "\n",
    "```shell\n",
    "$ docker run -p 8888:8888 jupyter/scipy-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go into the bash of the Docker container:\n",
    "\n",
    "```shell\n",
    "$ docker ps\n",
    "$ docker exec -it CONTAINER_ID bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are inside, you can install [pydataverse](https://github.com/AUSSDA/pyDataverse). To have the latest features, we install from the develop branch.\n",
    "\n",
    "```shell\n",
    "$ pip install git+https://github.com/aussda/pyDataverse.git@develop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can download the [pyDataverse templates from GitHub](https://github.com/AUSSDA/pyDataverse_templates), which are needed for the import:\n",
    "\n",
    "```shell\n",
    "$ cd work/\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_templates.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we download the [GitHub Repository for this workshop](https://github.com/AUSSDA/pyDataverse_workshop_tromso), with the Jupyter Notebook inside, prepared for this workshop.\n",
    "\n",
    "```shell\n",
    "$ git clone https://github.com/AUSSDA/pyDataverse_workshop_tromso.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* SCREENSHOT Command, Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open Jupyter Notebook `localhost:8888`.\n",
    "* Open `work/pyDataverse_workshop_tromso/pydataverse.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is installed and up and running, so we can move on to get our hands on some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. pyDataverse templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "* Show templates empty\n",
    "* show prepared dataset.csv and datafile.csv\n",
    "* Beziehung Datasets und Datafiles erklären\n",
    "* Text abändern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we added some data to the pyDataverse template files (datasets.csv, datafiles.csv), we can import the containing data into pyDataverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Dataset Metadata from templates to pyDataverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the needed Python modules.\n",
    "2. Convert the CSV data to a Python dictionary\n",
    "3. Create the pyDataverse Dataset object\n",
    "4. Print out metadata as json string\n",
    "5. Print out specific metadata variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import json\n",
    "from pyDataverse.api import Api\n",
    "from pyDataverse.models import Datafile\n",
    "from pyDataverse.models import Dataset\n",
    "from pyDataverse.utils import read_csv_to_dict\n",
    "from pyDataverse.utils import read_file\n",
    "from pyDataverse.utils import read_json\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filename = 'datasets.csv'\n",
    "license_filename = 'license.html'\n",
    "terms_filename = 'terms-of-access.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "license_default = read_file(license_filename)\n",
    "datasets_csv = read_csv_to_dict(ds_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_csv:\n",
    "    ds_tmp = {}\n",
    "    \n",
    "    ds_tmp['termsOfAccess'] = read_file(terms_filename)\n",
    "    for key, val in dataset.items():\n",
    "        if not val == '':\n",
    "            if key == 'aussda.dataset_id':\n",
    "                ds_id = val\n",
    "            elif key == 'dataverse.title':\n",
    "                ds_tmp['title'] = val\n",
    "            elif key == 'dataverse.subtitle':\n",
    "                ds_tmp['subtitle'] = val\n",
    "            elif key == 'dataverse.author':\n",
    "                ds_tmp['author'] = json.loads(val)\n",
    "            elif key == 'dataverse.dsDescription':\n",
    "                ds_tmp['dsDescription'] = []\n",
    "                ds_tmp['dsDescription'].append({'dsDescriptionValue': val})\n",
    "            elif key == 'dataverse.keywordValue':\n",
    "                ds_tmp['keyword'] = json.loads(val)\n",
    "            elif key == 'dataverse.topicClassification':\n",
    "                ds_tmp['topicClassification'] = json.loads(val)\n",
    "            elif key == 'dataverse.language':\n",
    "                ds_tmp['language'] = json.loads(val)\n",
    "            elif key == 'dataverse.subject':\n",
    "                ds_tmp['subject'] = []\n",
    "                ds_tmp['subject'].append(val)\n",
    "            elif key == 'dataverse.kindOfData':\n",
    "                ds_tmp['kindOfData'] = json.loads(val)\n",
    "            elif key == 'dataverse.datasetContact':\n",
    "                ds_tmp['datasetContact'] = json.loads(val)\n",
    "    data[ds_id] = {'metadata': ds_tmp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1 = Dataset()\n",
    "ds_1.set(data['test_1']['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_1.title)\n",
    "print(ds_1.dsDescription[0]['dsDescriptionValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Dataset Metadata via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get an API token for the Dataverse API. Go to [localhost:8085]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_alias = 'root'\n",
    "BASE_URL = 'http://localhost:8085'\n",
    "API_TOKEN = '714d16d5-c375-4051-a173-bc7d29fc0799'\n",
    "# API_TOKEN = 'SECRET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Api(BASE_URL, API_TOKEN)\n",
    "resp = api.get_dataverse(dv_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dsid2pid = {}\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    ds = Dataset()\n",
    "    ds.set(dataset['metadata'])\n",
    "    resp = api.create_dataset(dv_alias, ds.json())\n",
    "    pid = resp.json()['data']['persistentId']\n",
    "    mapping_dsid2pid[ds_id] = pid\n",
    "    time.sleep(1)\n",
    "    print('{0}/dataset.xhtml?persistentId={1}&version=DRAFT'.format(BASE_URL, pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Datafile metadata from templates to pyDataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filename = 'datafiles.csv'\n",
    "datafiles_csv = read_csv_to_dict(df_filename)\n",
    "\n",
    "for datafile in datafiles_csv:\n",
    "    df_tmp = {}\n",
    "    df_id = None\n",
    "    ds_id = None\n",
    "    for key, val in datafile.items():\n",
    "        if not val == '':\n",
    "            if key == 'dataverse.description':\n",
    "                df_tmp['description'] = val\n",
    "            elif key == 'aussda.filename':\n",
    "                df_tmp['filename'] = val\n",
    "            elif key == 'aussda.datafile_id':\n",
    "                df_tmp['datafile_id'] = val\n",
    "                df_id = val\n",
    "            elif key == 'aussda.dataset_id':\n",
    "                ds_id = val\n",
    "                df_tmp['dataset_id'] = ds_id\n",
    "            elif key == 'dataverse.categories':\n",
    "                df_tmp['categories'] = json.loads(val)\n",
    "    if 'datafiles' not in data[ds_id]:\n",
    "        data[ds_id]['datafiles'] = {}\n",
    "    if df_id not in data[ds_id]['datafiles']:\n",
    "        data[ds_id]['datafiles'][df_id] = {}\n",
    "    if 'metadata' not in data[ds_id]['datafiles'][df_id]:\n",
    "        data[ds_id]['datafiles'][df_id]['metadata'] = {}\n",
    "    data[ds_id]['datafiles'][df_id]['metadata'] = df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = Datafile()\n",
    "df_1.set(data['test_1']['datafiles']['1']['metadata'])\n",
    "df_1.set({'pid': mapping_dsid2pid['test_1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1.pid)\n",
    "print(df_1.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping_dsid2pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Datafile metadata via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    for df_id, datafile in dataset['datafiles'].items():\n",
    "        data_tmp = datafile['metadata']\n",
    "        data_tmp['pid'] = pid\n",
    "        df = Datafile()\n",
    "        df.set(data_tmp)\n",
    "        filename = os.path.abspath(os.path.join('data', datafile['metadata']['filename']))\n",
    "        path = api.native_api_base_url\n",
    "        path += '/datasets/:persistentId/add?persistentId={0}'.format(pid)\n",
    "        shell_command = 'curl -H \"X-Dataverse-key: {0}\"'.format(API_TOKEN)\n",
    "        shell_command += ' -X POST {0} -F file=@{1}'.format(path, filename)\n",
    "        shell_command += \" -F 'jsonData={0}'\".format(df.json())\n",
    "        result = sp.run(shell_command, shell=True, stdout=sp.PIPE)\n",
    "        if filename[-4:] == '.sav' or filename[-4:] == '.dta':\n",
    "            time.sleep(20)\n",
    "        else:\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publish Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    resp = api.publish_dataset(pid, 'major')\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delete Datasets via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    resp = api.delete_dataset(pid)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Dataverse API Docs](http://guides.dataverse.org/en/latest/api/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
